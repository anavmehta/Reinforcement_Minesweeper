!pip install tensorflow-gpu==1.15
!pip install  mss
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.python.ops.nn import relu, softmax
from tensorflow import keras
import gym
import sys
import os

import math
import numpy as np
import random
import time
import mss
from tkinter import *
from tkinter import font
import IPython

from random import randint





class MinesweeperEnv(object):
    def __init__(self, ROWS = 10, COLS = 10, SIZEOFSQ = 100, MINES_MIN = 13, MINES_MAX=15, display = False, 
                #rewards = {"win" : 2, "loss" : -2, "progress" : 1, "noprogress" : -1, "YOLO" : -1}    # Intially, when not learned
                #rewards = {"win" : 2, "loss" : -10, "progress" : 1, "noprogress" : -1, "YOLO" : -1}    # Intially, when not learned
                #rewards = {"win" : 2, "loss" : -10, "progress" : 1, "noprogress" : -1, "YOLO" : -1}  # For later, when learned game
                #rewards = {"win" : 10, "loss" : -1, "progress" : 0.9, "noprogress" : -0.3, "YOLO" : -0.3} # This is Jacob
                rewards = {"win" : 1, "loss" : -1, "progress" : 0.9, "noprogress" : -0.3, "YOLO" : -0.3} # This is Jacob
            ):
        """ Initialize Minesweeper
            Rows, Cols: int  - Number of rows and cols on the board
            SIZEOFSQ: pixels -  Determines the size of the window, reduce to get smaller window
            Mines: integer - Number of mines generated on the board
            display: bool - chooses weather to display the game with pygame
        """

        self.ROWS = ROWS
        self.COLS = COLS
        self.FULL = False
        self.MINES_MIN = MINES_MIN 
        self.MINES_MAX = MINES_MAX 
        self.MINES = 0

        self.display = display
        self.rewards = rewards

        self.grid = np.zeros((self.ROWS, self.COLS), dtype=object)
        self.state = np.zeros((self.ROWS, self.COLS), dtype=object)
        self.state_float = np.zeros((self.ROWS, self.COLS), dtype=float)
        self.state_last = np.copy(self.state)

        self.nbMatrix = np.zeros((ROWS, COLS), dtype=object)

        self.computeNeighbors() #Fills out the nbMatrix

        self.won = 0
        self.lost = 0
        if display: #Load pygame stuff

            #Scale to resolutions
            with mss.mss() as sct:
                img = np.array(sct.grab(sct.monitors[1]))
                self.SIZEOFSQ = int(SIZEOFSQ * img.shape[1] / 3840)
                SIZEOFSQ = self.SIZEOFSQ

            self.root = Tk()
            self.C = Canvas(self.root, bg="white", height= COLS * SIZEOFSQ - 1, width = ROWS * SIZEOFSQ - 1)



        self.initGame()

        if display:
            self.drawState()


    def drawState(self):
        c1 = "#0285DF"
        c2 = "#0491DF" 
        #Draw checked pattern
        for row in range(self.ROWS):
            for col in range(self.COLS):
                if self.checkpattern(col, row):
                    c = c1
                else:
                    c = c2

                self.C.create_rectangle(col*self.SIZEOFSQ,row*self.SIZEOFSQ, col*self.SIZEOFSQ + self.SIZEOFSQ ,row*self.SIZEOFSQ + self.SIZEOFSQ, fill=c, width=0)

        #Draw state
        for row in range(self.ROWS):
            for col in range(self.COLS):
                cell = self.state[row][col]
                if cell == 'E' or cell != 'U':
                    if self.checkpattern(col,row):
                        c = "#F2F4F7"
                    else:
                        c = "#F7F9FC"

                    self.C.create_rectangle(col*self.SIZEOFSQ,row*self.SIZEOFSQ, col*self.SIZEOFSQ + self.SIZEOFSQ ,row*self.SIZEOFSQ + self.SIZEOFSQ, fill=c, width=0)
                
                if cell != 'U' and cell !='E':    
                    if cell == 1:
                        c2 = "#00CC00"
                    elif cell == 2:
                        c2 = "#FFCC00"
                    elif cell == 3:
                        c2 = "#CC0000"
                    elif cell == 4:
                        c2 = "#003399"
                    elif cell == 5:
                        c2 = "#FF6600"
                    elif cell == 6:
                        c2 = "#FF6600"
                    elif cell == 'flag':
                        c2 = "#FF0000"


                    #num = np.random.randint(len(font.families()))
                    #print("({},{}) : {}".format(row,col,num))
                    f = self.C.create_text(col*self.SIZEOFSQ + int(0.5*self.SIZEOFSQ),row*self.SIZEOFSQ + int(0.5*self.SIZEOFSQ), \
                        font=('Nimbus Sans L', 24, "bold"), fill = c2) #101 pretty good font
                    self.C.itemconfigure(f, text=str(cell))

        self.C.pack()
        #self.root.mainloop()


    def initGame(self):
        self.grid = self.initBoard(startcol = 2, startrow = 2)
        self.state = np.ones((self.ROWS, self.COLS), dtype=object) * 'U'
        self.state_last = np.copy(self.state)


        self.action((2,2)) #Hack alert, to start off with non empty board. Can be removed but then agent has to learn
                         #what to do when the board starts out empty. 

    def initBoard(self, startcol, startrow):
        """ Initializes the board """

        COLS = self.COLS
        ROWS = self.ROWS
        grid = np.zeros((self.ROWS, self.COLS), dtype=object)
        #mines = self.MINES
        self.MINES = randint(self.MINES_MIN, self.MINES_MAX)
        mines = self.MINES

        #print("Initializing board with mines")
        #print(mines)


        #Randomly place bombs
        while mines > 0:
            (row, col) = (random.randint(0, ROWS-1), random.randint(0, COLS-1))
            #if (col,row) not in findNeighbors(startcol, startrow, grid) and grid[col][row] != 'B' and (col, row) not in (startcol, startrow):
            if (row,col) not in self.nbMatrix[startrow, startcol] and (row,col) != (startrow, startcol) and grid[row][col] != 'B':
                #print("Placing bombs")
                grid[row][col] = 'B'
                mines = mines - 1

        #Get rest of board when bombs have been placed
        for col in range(COLS):
            for row in range(ROWS):
                if grid[row][col] != 'B':
                    totMines = self.sumMines(col, row, grid)
                    if totMines > 0:
                        grid[row][col] = totMines
                    else:
                        grid[row][col] = 'E'


        return grid

    def computeNeighbors(self):
        """ Computes the neighbor matrix for quick lookups"""

        for row in range(self.ROWS):
            for col in range(self.COLS):
                self.nbMatrix[row][col] = self.findNeighbors(row, col)



    def findNeighbors(self, rowin, colin):
        """ Takes col, row and grid as input and returns as list of neighbors
        """
        COLS = self.grid.shape[1]
        ROWS = self.grid.shape[0]
        neighbors = []
        for col in range(colin-1, colin+2):
            for row in range(rowin-1, rowin+2):
                if (-1 < rowin < ROWS and 
                    -1 < colin < COLS and 
                    (rowin != row or colin != col) and
                    (0 <= col < COLS) and
                    (0 <= row < ROWS)):
                    neighbors.append((row,col))

        return neighbors


    def sumMines(self, col, row, grid):
        """ Finds amount of mines adjacent to a field.
        """
        mines = 0
        neighbors = self.nbMatrix[row, col]
        for n in neighbors:
            if grid[n[0],n[1]] == 'B':
                mines = mines + 1
        return mines


    def printState(self):
        """Prints the current state"""
        grid = self.state
        COLS = grid.shape[1]
        ROWS = grid.shape[0]
        for row in range(0,ROWS):
            print(' ')
            for col in range(0,COLS):
                print(grid[row][col], end=' ')


    def printBoard(self):
        """Prints the board """
        grid = self.grid
        COLS = grid.shape[1]
        ROWS = grid.shape[0]
        for row in range(0,ROWS):
            print(' ')
            for col in range(0,COLS):
                print(grid[row][col], end=' ')


    def reveal(self, col, row, checked, press = "LM"):
        """Finds out which values to show in the state when a square is pressed
           Checked : np.array((row,col)) to check which squares has already been checked
                     If the field is not a bomb we want to reveal it, if the field is empty
                     we want to find it's neighbors and reveal them too if they are not a bomb.   
        """
        if press == "LM":
            if checked[row][col] != 0:
                return
            checked[row][col] = checked[row][col] + 1
            if self.grid[row][col] != 'B':

                #Reveal to state space
                self.state[row][col] = self.grid[row][col]

                if self.grid[row][col] == 'E':
                    neighbors = self.findNeighbors(row, col)
                    for n in neighbors:
                        if not checked[n[0],n[1]]: 
                            self.reveal(n[1], n[0], checked)
        
        elif press == "RM":
            #Draw flag, not used for agent
            pass



    def action(self, a):
        """ External action, taken by human or agent
            row,col: integer - where the agent want to press
        """

        #print("Taking action")
        
        #If press a bomb game over, start new game and return bad reward, -10 in this case
        row, col = a[0], a[1]
        if self.grid[row][col] == "B":
            self.lost += 1
            #self.initGame()
            if self.display:
                print("Lost game")
            return({"s" : np.copy(self.state), "r" : self.rewards['loss'], "d" : True})

        #Take action and reveal new state
        self.reveal(col, row , np.zeros_like(self.grid))
        if self.display == True:
            self.drawState()

        #Winning condition
        if np.sum(self.state == "U") == self.MINES:
            self.won += 1
            #self.initGame()
            if self.display:
                print("Won game")
            return({"s" : np.copy(self.state), "r" :  self.rewards['win'], "d" : True})

        #Get the reward for the given action
        reward = self.compute_reward(a)

        #return the state and the reward
        return({"s" : np.copy(self.state), "r" : reward, "d" : False})


    def compute_reward(self, a):
        """Computes the reward for a given action"""

        #Reward = 1 if we get less unknowns, 0 otherwise 
        if (np.sum(self.state_last == 'U') - np.sum(self.state == 'U')) > 0:
            reward =  self.rewards['progress']
        else:
            reward =  self.rewards['noprogress']

        #YOLO -> it it clicks on a random field with unknown neighbors
        tot = 0
        for n in self.nbMatrix[a[0],a[1]]:
            if self.state_last[n[0],n[1]] == 'U':
                tot += 1
        if tot == len(self.nbMatrix[a[0],a[1]]):
            reward = self.rewards['YOLO']


        self.state_last = np.copy(self.state)
        return(reward)
            


    def checkpattern(self, col, row):
        #Function to construct the checked pattern in pygame
        if row % 2:
            if col % 2: #If unequal
                return True
            else: #if equal
                return False
        else: 
            if col % 2: #If unequal
                return False
            else: #if equal
                return True

    def initPattern(self):
        #Initialize pattern:

        c1 = "#0285DF"
        c2 = "#0491DF"          
        rects = []
        for row in range(self.ROWS):
            for col in range(self.COLS):
                if self.checkpattern(col, row):
                    c = c1
                else:
                    c = c2

                self.C.create_rectangle(col*self.SIZEOFSQ,row*self.SIZEOFSQ, col*self.SIZEOFSQ + self.SIZEOFSQ ,row*self.SIZEOFSQ + self.SIZEOFSQ, fill=c)
                self.C.pack()
                self.root.mainloop()


    def stateConverter(self, state):
        """ Converts 2d state to one-hot encoded 3d state
            input: state (rows x cols)
            output: state3d (row x cols x 10) (if full)
                            (row x cols x 2) (if not full)
        """
        rows, cols = state.shape
        if self.FULL:
            res = np.zeros((rows,cols,10), dtype = int)
            for i in range(0,8):
                res[:,:,i] = state == i+1 #1-7
            res[:,:,8] = state == 'U'
            res[:,:,9] = state == 'E'
           
            return(res)
        else:
            #res = np.ones((rows, cols, 2)) * -1
            #filtr = ~np.logical_or(state == "U", state == "E") #Not U or E
            #res[filtr,0] = state[filtr] / 10
            #res[state == "U", 1] = 1

            res = np.zeros((rows, cols, 2))
            filtr = ~np.logical_or(state == "U", state == "E") #Not U or E
            res[filtr,0] = state[filtr]
            res[state == "U", 1] = 1
            return(res)


    def get_state(self):

        for row in range(self.ROWS):
            for col in range(self.COLS):
                field = self.state[row][col]
                if type(field) == int:
                    self.state_float[row][col]= field*20
                elif field == 'U':
                    self.state_float[row][col] = -ord('U')/2
                else:
                    self.state_float[row][col] = -ord('E')/4

        return np.copy(self.state_float)/200


    # Wrap to openai gym API
    def step(self, a):
        a = np.unravel_index(a, (self.ROWS,self.COLS))
        d = self.action(a)
        #d["s"] = np.reshape(self.stateConverter(d["s"]),(self.ROWS*self.COLS*2))

        # For 6x6x2
        d["s"] = self.stateConverter(d["s"])
        # For 6x6x1
        #d["s"] = self.get_state()
        return d["s"], d["r"], d["d"], None

    def reset(self):
        self.initGame()

        # For 6x6x2
        return self.stateConverter(self.state)
        # For 6x6x1
        #return self.get_state()



    # def step(self, a):
    #     a = np.unravel_index(a, (self.ROWS,self.COLS))
    #     d = self.action(a)
    #     d["s"] = self.get_state()
    #     return d["s"], d["r"], d["d"], None

    # def reset(self):
    #     self.initGame()
    #     return self.get_state()

"""
This module contains class definitions for open ai gym environments.
"""

import random
import gym

# from minesweeper_env import MinesweeperEnv
import numpy as np


class BaseEnvironment(object):
    """A base environment class for open ai gym.
    Attributes
    ----------
    _episode_step : int 
        The number of steps in the current episode (game).
    _episode_number : int
        The number of games played.
    _episode_reward : int
        The score in the current episode.
    _max_reward_episode : float
        The maximum reward per episode.
    _global_step : int
        The total number of steps globally.
    _global_reward : float
        The total score over all games.
    """


    def __init__(self):

        self._episode_step       = 0
        self._episode_number     = 0
        self._episode_reward     = 0.0
        self._max_reward_episode = 0.0
        self._global_step        = 0
        self._global_reward      = 0.0
        self._recent_reward      = 0.0
        self._recent_episode_number = 0.0


    def avg_reward_per_episode(self):
        """Computes the average reward per episode.
        Returns
        -------
        float
            The average reward per episode.
        """
        return self.global_reward/(self._episode_number+1)


    def avg_reward_per_episode_recent(self):
        """Computes the average reward per most recent episodes/games
        Returns
        -------
        float
            The average reward per episode.
        """

        return self._recent_reward/(self._recent_episode_number+1)


    def avg_steps_per_episode(self):
        """Computes the average number of steps per episode.
        Returns
        -------
        float
            The average number of steps per episode.
        """
        return self.global_step/(self._episode_number+1)


    @property
    def episode_number(self):
        """int: The current episode number."""
        return self._episode_number


    @property
    def episode_step(self):
        """int: The number of steps in the current episode."""
        return self._episode_step


    @property
    def global_step(self):
        """int: The total number of steps globally."""
        return self._global_step


    @property
    def episode_reward(self):
        """float: The score in the current episode."""
        return self._episode_reward


    @property
    def global_reward(self):
        """float: The total score over all episodes."""
        return self._global_reward


    @property
    def max_reward_per_episode(self):
        """float: The maximum reward per episode."""
        return self._max_reward_episode


class MinesweeperEnvironment(BaseEnvironment):

    """An environment class for open ai gym atari games using the screen.
    Parameters
    ----------
    random_start : int
        The maximum random number of steps to take in an environment initially.
    display : Bool
        If true, show the game being played.
    game : str
        The selected game.
    Attributes
    ----------
    _display : bool
        If true, show the game being played.
    _screen_rgb : :obj: 'ndarray' of :obj: 'float'
        The screen output (rgb)
    _reward : float
        The amount of reward achieved by the previous action. 
    _done : bool
        Whether it is time to reset the environment again. 
        Most (but not all) tasks are divided up into well-defined episodes, 
        and done being True indicates the episode has terminated.
    _random_start : int
        How long we let the agent take random actions in a new game.
    screen_height : int
        The height of the screen after resizing.
    screen_width : int
        The width of the screen after resizing.
    _action_repeat : int
        The number of time-steps an action is repeated.
    env : :obj:
        The open ai gym environment object.
    """


    def __init__(
        self, rows = 10, cols = 10, mines_min = 13, mines_max=15, display = False, reward_recent_update = 1000
    ):

        # Constructor of base class
        super(MinesweeperEnvironment, self).__init__() # No arguments to init in
                                                    # BaseEnvironment

        self._mines_min = mines_min
        self._mines_max = mines_max

        self._screen    = None
        self._display       = display
        self._reward        = 0.0
        self._done          = False
        self._random_start  = 10
        self.reward_recent_update = reward_recent_update

        self._games_won = 0
        self._recent_games_won = 0

        self.screen_height  = rows
        self.screen_width   = cols

        self._action_repeat = 1

        # Initialize Gym Environment with selected game
        self.env = MinesweeperEnv(self.screen_height, self.screen_width, 100, self._mines_min, self._mines_max, self._display)

    def new_game(self):
        """Creates a new game.
        Returns
        -------
        int
            0
        :obj: 'ndarray' of :obj: 'float'
            The current screen in grayscale and resized
        bool
            True for game is over, False otherwise
        """
        self._max_reward_episode = max(self._max_reward_episode, 
                                        self._episode_reward)

        self._episode_step   = 0
        self._episode_reward = 0.0

        # The number of games played
        self._episode_number += 1 # Since we may have ended a previous episode
                                  # and started a new game

        self._recent_episode_number += 1


        if not (self._recent_episode_number + 1) % self.reward_recent_update: # We count the most recent reward
            print("Resetting recent reward counter")
            self._recent_reward = 0
            self._recent_episode_number = 0
            self._recent_games_won = 0

        self._screen = self.env.reset()
        self.render()
        return self.state


    def new_random_game(self):
        """Creates a new game where we initially perform some random actions
           to get a different start to the game.
        Returns
        -------
        int
            0.
        :obj: 'ndarray' of :obj: 'float'
            The current screen in grayscale and resized.
        bool
            True for game is over, False otherwise.
        """

        return self.new_game()


    def _step(self, action):
        """Takes a step in the environment by giving an action and updating
        all related attributes.
        Returns
        -------
        None
        """

        res = self.env.step(action)
        self._screen = res[0]
        self._reward = res[1]
        self._done = res[2]
        self._episode_step += 1
        self._global_step  += 1
        self._episode_reward += self._reward
        self._global_reward  += self._reward

        self._recent_reward += self._reward

        if self._reward == self.env.rewards["win"]:
            self._games_won += 1
            self._recent_games_won += 1

    def _random_step(self):
        action = self.act_random()
        self._step(action)



    def after_act(self, action):
        """Actions to perform after an environment action.
        Returns
        -------
        None
        """

        self.render()


    def act(self, action, is_training=True):
        """Perform an action in an environment with both a step and whatever
        extra is defined in after_act().
        Returns
        -------
        :obj: 'ndarray' of :obj: 'float'
            state after action has been performed
        """

        # https://github.com/openai/gym/blob/master/gym/envs/atari/atari_env.py
        # The Gym environment for Breakout performs the following _step():
        #   num_steps = self.np_random.randint(2, 5)
        #   for _ in range(num_steps):
        #      reward += self.ale.act(action)

        self._step(action)
        self.after_act(action)
        return self.state


    def act_random(self):
        return np.random.randint(0, self.screen_width*self.screen_height-1, size=1)[0]


    def render(self):
        """Render the environment.
        Returns
        -------
        None
        """
        if self._display:
            self.env.drawState()


    @property
    def state(self):
        """:obj: 'list' of 
                :obj: 'ndarray' of :obj: float, 
                :obj: float, 
                :obj: bool: 
        Returns the state of the environment.
        """
        return self.screen, self._reward, self._done


    @property
    def screen(self):
        return self._screen

    @property
    def win_rate(self):
        return self._games_won / (self._episode_number+1)


    @property
    def win_rate_recent(self):
        return self._recent_games_won / (self._recent_episode_number+1)


    @property
    def num_actions(self):
        """int: The number of actions available in the selected game."""
        return int(self.screen_width*self.screen_height)


    @property
    def legal_actions(self):
        """int: Available legal actions."""
        return int(self.screen_width*self.screen_height)

    def action_to_string(self, action_id):
        """str: The meanings of actions."""
        return 'Nope'

"""
This module contains a function to update another neural network by copying
all trainable variables and a class for a deep Q network.
"""

import os
import numpy as np
import tensorflow as tf

# Based on
# https://github.com/tomrunia/DeepReinforcementLearning-Atari

def update_target_network(sess, network_name_train, network_name_target):
    """This helper method copies all the trainable weights and biases from
    one DeepQNetwork to another. This method is used for synchronisation
    of the train and target Q-networks
    Parameters
    ----------
    sess: tensorflow session.
        The current session in tensorflow.
    network_name_train: str
        Name of Q-Network for training.s
    network_name_target: str
        Name of Q-Network for predicting target Q-values
    Returns
    -------
    None
    """

    # Retrieve current variable scope and set reuse flag to True
    tf.get_variable_scope().reuse_variables()

    # Get list of values in "network_name_train" in the collection
    # GraphKeys.TRAINABLE_VARIABLES
    vars_source = tf.get_collection(
        tf.GraphKeys.TRAINABLE_VARIABLES, scope=network_name_train
    )

    copy_ops  = [] # Where we copy the variables to
    check_ops = [] # Contains a list of booleans determining if variables are equal

    for v in vars_source:
        # Note the [0:-2] to cut of the device placement
        v_source = tf.get_variable(v.name[0:-2])

        # Remove variable prefix (network name)
        var_name = v.name[v.name.find("/"):]
        v_target = tf.get_variable(network_name_target + var_name[0:-2])

        copy_ops.append(v_target.assign(v_source))
        check_ops.append(tf.equal(v_target, v_source))

    # Actual copying all the variables, check if the values are equal
    sess.run(copy_ops)
    check_res = sess.run(check_ops)
    for res in check_res:
        if not np.all(res): # Failed to copy variables
            raise ValueError("Verification of tf.equal(var_train, var_target) failed.")


class DeepQNetwork:
    """A class for replay memories used during training of deep Q-networks.
    Parameters
    ----------
    replay_capacity : int
        The maximum number of samples in replay memory
    batch_size : Bool
        The number of frames to return.
    Attributes:
        network_name : str
            The name of the  network.
        trainable : bool
            If this network is supposed to be trained.
            True for the Q-network we train and False for the targer Q-network.
        model_file : str
            The name of the file containing the trained model parameters.
        output_dir : str
            The name of the output directory.
        input_size : int
            The size of the screen input.
        input_depth : int
            The input depth, this is the number of frames used, expects 4.
        filter_sizes: :obj: 'list' of :obj: 'int'
            A list containing the filter sizes in conv layers.
        filter_strides: :obj: 'list' of :obj: 'int'
            A list containing the filter strides in conv layers.
        num_filters: :obj: 'list' of :obj: 'int'
            A list containing the number of filters in conv layers.
        num_hidden: :obj: 'list' of :obj: 'int'
            A list containing the number of hidden units in feedforward layers.
        num_actions: int
            The number of actions available.
        dueling_type: str
            The type of dueling.
        batch_size: int
            Size of the training batch.
        learning_rate_base: float
            The start learning rate.
        learning_rate_decay: float
            The decay rate of the learning rate. 
            We decay every larning_rate_iteration.
        learning_rate_step: int
            The number of iterations to anneal the learning rate.
        learning_rate_min: float
            The minimum learning rate.
        clip_delta: bool
            If True use error clipping.
        batch_accumulator: str
            Either "mean" or "sum", determines if we reduce sum or mean of
            delta_square
    """


    def __init__(self, params, num_actions, network_name, trainable):

        self.params = params

        # Network information
        self.network_name        = network_name
        self.trainable           = trainable

        # Paths and models
        self.model_file          = params.model_file
        self.output_dir          = params.output_dir

        # Input to the Q-network
        self.input_height         = params.input_height      # 84
        self.input_width          = params.input_width      # 84

        self.env_type = params.env

        if self.env_type == 'minesweeper':
            self.input_depth          = params.nchannels  # 4
        else:
            self.input_depth          = params.history_length  # 4


        self.network_type = params.network_type

        # CNN layer architecture
        self.filter_sizes        = params.filter_sizes    # 8, 4, 3
        self.filter_strides      = params.filter_strides  # 4, 2, 1
        self.num_filters         = params.num_filters     # 32, 64, 64
        self.num_hidden          = params.num_hidden      # 512
        self.num_actions         = num_actions            # depends on game
        self.dueling_type        = params.dueling_type

        # Training Parameters
        self.batch_size          = params.batch_size             # 32
        self.learning_rate_base  = params.learning_rate          # 0.0025
        self.learning_rate_decay = params.learning_rate_decay    # 0.96
        self.learning_rate_step  = params.learning_rate_step     # 450000
        self.learning_rate_min   = params.learning_rate_minimum  # 0.00025
        self.discount            = params.discount               # 0.99
        self.network_update_rate = params.network_update_rate    # 10000
        self.clip_delta          = params.clip_delta             # 1.0
        self.batch_accumulator   = params.batch_accumulator      # mean or sum

        # Init TensorFlow graph
        self._init_graph()

    def _init_graph(self):
        """Initializes the computational graph in Tensorflow and builds the
        neural network.
        If trainable is True, we also build the loss part of the graph.
        Returns
        -------
        None
        """

        # Initialize operation for creating/writing summary to disk
        self.train_summary_op     = None
        self.eval_summary_op      = None
        self.train_summary_writer = None

        with tf.variable_scope(self.network_name):

            self.global_iteration    = tf.Variable(0, name='global_iteration', 
                                    trainable=False)

            # Placeholders
            #tf.enable_eager_execution()
            #tf.executing_eagerly()
            #tf.disable_eager_execution()
            #tf.disable_v2_behavior()
            self.pl_screens = tf.placeholder(tf.float32, 
                                    shape=[None, 
                                            self.input_height, 
                                            self.input_width, 
                                            self.input_depth], 
                                            name="screens")

            # For feedforward networks
            self.pl_flat = tf.placeholder(tf.float32, 
                                    shape=[None, 
                                            self.input_height*self.input_width*self.input_depth], 
                                            name="flat")


            # Contain q-targest for all batches per action, None for batches
            self.pl_qtargets = tf.placeholder(tf.float32, shape=[None, self.num_actions], 
                                    name="qtargets")

            self.pl_actions = tf.placeholder(tf.int64, shape=[None], 
                                    name="actions")

            self.pl_actions_target = tf.placeholder(tf.int64, shape=[None],  # Actions from train-network used to select q-values in target network
                                    name="actions_target")

            self.pl_rewards = tf.placeholder(tf.float32, shape=[None], 
                                    name="rewards")

            self.pl_dones   = tf.placeholder(tf.float32, shape=[None], 
                                    name="dones")

            # Convert strings to lists
            self.filter_sizes   = list(map(int, self.filter_sizes.split(",")))
            self.filter_strides = list(map(int, self.filter_strides.split(",")))
            self.num_filters    = list(map(int, self.num_filters.split(",")))

            # Initialize the CNN
            if self.network_type == 'conv':
                self.qvalues = self._build_cnn()
            elif self.network_type == 'fc':
                self.qvalues = self._build_fc()

            # Initialize the loss function if we have the trainable Q-network.
            # Otherwise all parameters are copied.
            if self.trainable:

                self._build_loss()

                # Learning rate decay
                # When training a model, it is often recommended to lower the 
                # learning rate as the training progresses. 
                # This function applies an exponential decay function to a 
                # provided initial learning rate.
                # Uses global_iteration to compute the decayed learning rate.
                
                #tf.executing_eagerly()
                #tf.enable_eager_execution()
                self.learning_rate   = tf.train.exponential_decay(
                    self.learning_rate_base, self.global_iteration,
                    self.learning_rate_step, self.learning_rate_decay,
                    staircase=True
                )

                self.learning_rate = tf.maximum(self.learning_rate, 
                                        self.learning_rate_min)

                # Adam Optimizer
                self.optimizer = tf.train.AdamOptimizer(
                                    learning_rate=self.learning_rate,
                                    epsilon=1.5e-4) # From Rainbow paper

                self.train_op_optimizer = self.optimizer.minimize(
                                            self.loss, 
                                            global_step=self.global_iteration # Steps refer to something else in reinforcement, so we call iteration
                                        )

                # Keep track of running average of the loss when minimizing 
                # the loss
                # tf.control_dependencies means that 
                # self.train_op = tf.group(self.loss_moving_avg_op)
                # is only run after self.loss_moving_avg_op has been executed
                with tf.control_dependencies([self.train_op_optimizer]):
                    self.train_op = tf.group(self.loss_moving_avg_op)

    def _build_cnn(self):
        """Builds a convolutional network. When we add new layers, it is
        important to set trainable=self.trainable since these parameters needs
        to be copied to the target nework and it uses the set of trianable.
        Returns
        -------
        :obj: 'ndarray' of :obj: 'float'
            The q-values in the output as a list, corresponding to each possible
            action.
        """

        with tf.variable_scope("Network"):

            if self.env_type == 'atari':

                conv1 = tf.layers.conv2d(
                    inputs=self.pl_screens, 
                    filters=32, 
                    kernel_size=[8, 8],
                    strides=(4, 4),
                    activation=tf.nn.relu,
                    padding="valid",  # Same as in keras
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                    #kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                    bias_initializer= tf.constant_initializer(self.params.bias_init),
                    trainable=self.trainable,
                    name="conv1"
                    )

                conv2 = tf.layers.conv2d(
                    inputs=conv1, 
                    filters=64, 
                    kernel_size=[4, 4],
                    strides=(2, 2),
                    activation=tf.nn.relu,
                    padding="valid", 
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                    bias_initializer= tf.constant_initializer(self.params.bias_init),
                    trainable=self.trainable,
                    name="conv2"
                    )

                conv3 = tf.layers.conv2d(
                    inputs=conv2, 
                    filters=64, 
                    kernel_size=[3, 3],
                    strides=(1, 1),
                    activation=tf.nn.relu,
                    padding="valid", 
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                    bias_initializer= tf.constant_initializer(self.params.bias_init),
                    trainable=self.trainable,
                    name="conv3"
                    )

            elif self.env_type == 'minesweeper':
                print("Building small network")

                # Make the  network smaller

                conv1 = tf.layers.conv2d(
                    inputs=self.pl_screens, 
                    filters=18, 
                    kernel_size=[5, 5],
                    strides=(1, 1),
                    activation=tf.nn.relu,
                    padding="same",  # Same as in keras
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                    bias_initializer= tf.constant_initializer(self.params.bias_init),
                    trainable=self.trainable,
                    name="conv1"
                    )

                conv2 = tf.layers.conv2d(
                    inputs=conv1, 
                    filters=36, 
                    kernel_size=[3, 3],
                    strides=(1,1),
                    activation=tf.nn.relu,
                    padding="same",  # Same as in keras
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                    # kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                    bias_initializer= tf.constant_initializer(self.params.bias_init),
                    trainable=self.trainable,
                    name="conv2"
                    )

                # conv3 = tf.layers.conv2d(
                #     inputs=conv2, 
                #     filters=32, 
                #     kernel_size=[2, 2],
                #     strides=(1,1),
                #     activation=tf.nn.relu,
                #     padding="valid",  # Same as in keras
                #     kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                #     bias_initializer= tf.constant_initializer(self.params.bias_init),
                #     trainable=self.trainable,
                #     name="conv3"
                #     )


            # To implement dueling, see https://github.com/dbobrenko/reinforceflow/blob/master/reinforceflow/nets/dueling.py

            if self.dueling_type is None:

                conv_flatten = tf.layers.flatten(
                    inputs=conv2
                    )

                fc1 = tf.layers.dense(
                        inputs=conv_flatten, 
                        units=288, 
                        activation=tf.nn.relu, 
                        # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                        #kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name="fc1"
                    )

                fc2 = tf.layers.dense(
                        inputs=fc1, 
                        units=220, 
                        activation=tf.nn.relu, 
                        # kernel_initializer=tf.contrib.layers.xavier_initializer(),
                        #kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name="fc2"
                    )


                fc3 = tf.layers.dense(
                        inputs=fc2, 
                        units=220, 
                        activation=tf.nn.relu, 
                        # kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name="fc3"
                    )


                qvalues = tf.layers.dense(
                            inputs=fc3, 
                            units=self.num_actions, 
                            # kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                            bias_initializer= tf.constant_initializer(self.params.bias_init),
                            trainable=self.trainable
                        )

            # Dueling is enabled
            else:
                #We take the output from the final convolutional layer and 
                # split it into separate advantage and value streams.

                streamAC, streamVC = tf.split(conv3,2,3)

                streamA = tf.contrib.layers.flatten(streamAC)
                streamV = tf.contrib.layers.flatten(streamVC)

                qvalues = self._add_dueling(streamA, streamV)

        return qvalues


    def _build_fc(self):
        """Builds a fully-connected network. When we add new layers, it is
        important to set trainable=self.trainable since these parameters needs
        to be copied to the target nework and it uses the set of trianable.
        Returns
        -------
        :obj: 'ndarray' of :obj: 'float'
            The q-values in the output as a list, corresponding to each possible
            action.
        """

        with tf.variable_scope("Network"):

            if self.env_type == 'atari':
                pass

            elif self.env_type == 'minesweeper':
                print("Building small network")

                fc1 = tf.layers.dense(
                        inputs=self.pl_flat,
                        units=6*6*4,
                        activation=tf.nn.relu,
                        kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name='fc1'
                    )

                fc2 = tf.layers.dense(
                        inputs=fc1,
                        units=150,
                        activation=tf.nn.relu,
                        kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name='fc2'
                    )

                fc3 = tf.layers.dense(
                        inputs=fc2,
                        units=150,
                        activation=tf.nn.relu,
                        kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name='fc3'
                    )



            if self.dueling_type is None:

                fc4 = tf.layers.dense(
                        inputs=fc3, 
                        units=150, 
                        activation=tf.nn.relu, 
                        kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                        bias_initializer= tf.constant_initializer(self.params.bias_init),
                        trainable=self.trainable,
                        name="fc4"
                    )

                qvalues = tf.layers.dense(
                            inputs=fc4, 
                            units=self.num_actions, 
                            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
                            bias_initializer= tf.constant_initializer(self.params.bias_init),
                            trainable=self.trainable
                        )

            # Dueling is enabled
            else:

                qvalues = self._add_dueling(fc3, fc3)

        return qvalues

    def _add_dueling(self, streamA, streamV):
        """Adds dueling to a network.
        See:
        https://github.com/devsisters/DQN-tensorflow/blob/master/dqn/agent.py
        https://github.com/dbobrenko/reinforceflow/blob/master/reinforceflow/nets/dueling.py
        https://github.com/devsisters/DQN-tensorflow/blob/master/dqn/agent.py
        https://github.com/gokhanettin/dddqn-tf/blob/master/dddqn.py
        https://github.com/awjuliani/DeepRL-Agents/blob/master/Double-Dueling-DQN.ipynb
        And the original paper
        https://arxiv.org/pdf/1511.06581.pdf
        Parameters
        ----------
        streamA :obj: 'ndarray' of :obj: 'float'
                    The input to the advantage part.
        streamV :obj: 'ndarray' of :obj: 'float'
                    The input to value part.
        (Deprecated)
        net_input :obj: 'ndarray' of :obj: 'float'
            The input to the part of the network implementing dueling
        Returns
        -------
        :obj: 'ndarray' of :obj: 'float'
            The q-values in the output as a list, corresponding to each possible
            action.
        """

        adv_in = tf.layers.dense(
            inputs=streamA, 
            units=512, 
            activation=tf.nn.relu, 
            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
            bias_initializer= tf.constant_initializer(self.params.bias_init),
            trainable=self.trainable,
            name="adv_in")

        adv_out = tf.layers.dense(
            inputs=adv_in, 
            units=self.num_actions, 
            activation=None, 
            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
            bias_initializer= tf.constant_initializer(self.params.bias_init),
            trainable=self.trainable,
            name="adv_out")

        value_in = tf.layers.dense(
            inputs = streamV,
            units=512, 
            activation=tf.nn.relu, 
            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
            bias_initializer= tf.constant_initializer(self.params.bias_init),
            trainable=self.trainable,
            name="value_in")

        value_out = tf.layers.dense(
            inputs=value_in, 
            units=1, 
            activation=tf.nn.relu, 
            kernel_initializer=tf.contrib.layers.xavier_initializer(seed=self.params.seed),
            bias_initializer= tf.constant_initializer(self.params.bias_init),
            trainable=self.trainable,
            name="value_out")


        # caculate the Q(s,a;theta)
        # dueling_type == 'avg'
        # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-Avg_a(A(s,a;theta)))
        # dueling_type == 'max'
        # Q(s,a;theta) = V(s;theta) + (A(s,a;theta)-max_a(A(s,a;theta)))
        # dueling_type == 'naive'
        # Q(s,a;theta) = V(s;theta) + A(s,a;theta)

        if self.dueling_type == 'naive':

            qvalues = value_out + adv_out

        # Mean is what is suggested in the paper
        elif self.dueling_type == 'mean':

            qvalues = tf.add(value_out, 
                        tf.subtract(adv_out, 
                            tf.reduce_mean(adv_out, 1, keep_dims=True)))

        elif self.dueling_type == 'max':

            qvalues = tf.add(value_out, 
                        tf.subtract(adv_out, 
                            tf.reduce_max(adv_out, 1, keep_dims=True)))

        else:

            raise ValueError("Unknown dueling type '%s'. Available: 'naive', \
                        'mean', 'max'." % self.dueling_type)

        return qvalues

    def _build_loss(self):
        """Build the loss function for the neural network.
        """

        with tf.variable_scope("loss"):

            # Compute y_j = r_j + discount*max_qvalue
            self.tf_discount = tf.constant(self.discount)
            #self.pl_qtargets = tf.reduce_max(self.pl_qtargets, reduction_indices=[1])


            # self.pl_actions_train
            #self.pl_qtargets = self.pl_qtargets[range(self.batch_size), self.pl_actions_target]

            # Get Q-values for the argmax actions from actions train
            # For this, we use the one-hot encoding
            self.actions_one_hot_target = tf.one_hot(
                                    self.pl_actions_target, self.num_actions, 1.0, 0.0
                                    )

            # Finally, we compute the q-vaue of the target network
            # for the actions which maximixed the 
            self.qtarget = tf.reduce_sum(
                                    tf.multiply(
                                        self.pl_qtargets, 
                                        self.actions_one_hot_target), 
                                        reduction_indices=1
                                )

            if self.env_type == 'atari':

                # In atari, done is bad since the games should last as long as possible
                # for mine-sweeper, done is ambigious, and we represent loss by a strong negative
                # reward, and win is simply progressing by a certain number of steps
                self.qtarget = tf.add(
                                    self.pl_rewards, 
                                    tf.multiply(1.0-self.pl_dones, 
                                        tf.multiply(self.tf_discount, self.qtarget))
                                )

            elif self.env_type == 'minesweeper':

                self.qtarget = tf.add(
                                        self.pl_rewards, 
                                        tf.multiply(self.tf_discount, self.qtarget)
                                    )

            # Select Q-values for given actions
            # The actions are a 1D array, we one-hot encode with the number of actions
            # last two are on-value, off-value
            # Then we just have a matrix with 1s where an action was performed
            self.actions_one_hot = tf.one_hot(
                                    self.pl_actions, self.num_actions, 1.0, 0.0
                                    )


            # Below we obtain the loss by taking the sum of squares difference 
            # between the target and prediction Q values.
            # Compute finally sum along all rows by summing column elements
            self.qvalue_pred = tf.reduce_sum(
                                    tf.multiply(
                                        self.qvalues, 
                                        self.actions_one_hot), 
                                        reduction_indices=1
                                )
            
            # Difference between target (true) and predicted Q-network output
            self.delta = tf.subtract(self.qtarget, self.qvalue_pred)

            if self.clip_delta:
                # Perform clipping of the error term, 
                # default clipping is to (-1, +1) range
                self.delta_square = tf.losses.huber_loss(self.qtarget, self.qvalue_pred)
            else:
                # No error clipping
                # td-error
                self.delta_square  = tf.square(self.delta)

        # Actual loss
        if self.batch_accumulator == "sum":
           self.loss = tf.reduce_sum(self.delta_square)
        else:
           self.loss = tf.reduce_mean(self.delta_square)


        # Running average of the loss for TensorBoard
        self.loss_moving_avg    = tf.train.ExponentialMovingAverage(decay=0.999)
        self.loss_moving_avg_op = self.loss_moving_avg.apply([self.loss])


    def build_summary_writer(self, sess):
        """Build summary writer of the progress.
        """

        # Compute the average Q-value
        avg_qvalues_train  = tf.reduce_mean(self.qvalues)
        avg_qvalues_target = tf.reduce_mean(tf.reduce_mean(self.pl_qtargets))
        avg_reward_batch   = tf.reduce_mean(tf.reduce_mean(self.pl_rewards))

        # Summaries for training
        training_summaries = [
            tf.summary.scalar("train/qvalues_train_avg", avg_qvalues_train),
            tf.summary.scalar("train/qvalues_target_avg", avg_qvalues_target),
            tf.summary.scalar("train/avg_reward_batch", avg_reward_batch),
            tf.summary.scalar("train/loss", self.loss),
            tf.summary.scalar("train/loss_average", self.loss_moving_avg.average(self.loss)),
            tf.summary.scalar("train/learning_rate", self.learning_rate),
            tf.summary.histogram("train/delta", self.delta)
        ]
        training_summaries_merged = tf.summary.merge(training_summaries)

        # Environment related summaries
        with tf.variable_scope("environment"):
            self.avg_reward_per_game = tf.Variable(0.0, trainable=False, name="avg_reward_per_game")
            self.max_reward_per_game = tf.Variable(0.0, trainable=False, name="max_reward_per_game")
            self.avg_moves_per_game  = tf.Variable(0.0, trainable=False, name="avg_moves_per_game")
            self.num_games_played    = tf.Variable(0.0, trainable=False, name="num_games_played")
            self.moves               = tf.Variable(0.0, trainable=False, name="num_moves_played")
            self.total_reward_replay = tf.Variable(0.0, trainable=False, name="reward_in_replay_memory")
            self.actions_random      = tf.Variable(0.0, trainable=False, name="num_actions_random")
            self.actions_greedy      = tf.Variable(0.0, trainable=False, name="num_actions_greedy")

        environment_summaries = [
            tf.summary.scalar("environment/avg_reward_per_game", self.avg_reward_per_game),
            tf.summary.scalar("environment/max_reward_per_game", self.max_reward_per_game),
            tf.summary.scalar("environment/num_games_played", self.num_games_played),
            tf.summary.scalar("environment/moves", self.moves),
            tf.summary.scalar("environment/avg_moves_per_game", self.avg_moves_per_game),
            tf.summary.scalar("environment/reward_in_replay_memory", self.total_reward_replay),
            tf.summary.scalar("actions/num_actions_random", self.actions_random),
            tf.summary.scalar("actions/num_actions_greedy", self.actions_greedy),
            #tf.summary.image("screens", self.pl_screens, max_outputs=10) # This only works with atari for some reason
        ]
        environment_summaries_merged = tf.summary.merge(environment_summaries)

        # Environment related summaries
        with tf.variable_scope("evaluation"):
            self.eval_rewards      = tf.Variable(0.0, trainable=False, name="total_reward")
            self.eval_win_rate     = tf.Variable(0.0, trainable=False, name="win_rate")
            self.eval_num_rewards  = tf.Variable(0.0, trainable=False, name="num_rewards")
            self.eval_max_reward   = tf.Variable(0.0, trainable=False, name="max_reward")
            self.eval_num_episodes = tf.Variable(0.0, trainable=False, name="num_episodes")
            self.eval_actions      = tf.Variable(np.zeros(self.num_actions), trainable=False, name="actions")


        evaluation_summaries = [
            tf.summary.scalar("evaluation/total_reward", self.eval_rewards),
            tf.summary.scalar("evaluation/win_rate", self.eval_win_rate),
            tf.summary.scalar("evaluation/num_rewards", self.eval_num_rewards),
            tf.summary.scalar("evaluation/max_reward", self.eval_max_reward),
            tf.summary.scalar("evaluation/num_episodes", self.eval_num_episodes),
            tf.summary.histogram("evaluation/actions", self.eval_actions)
        ]

        # Evaluation Summaries for TensorBoard
        self.eval_summary_op = tf.summary.merge(evaluation_summaries)

        # Training summaries for TensorBoard
        self.train_summary_op = tf.summary.merge([training_summaries_merged,
                                                  environment_summaries_merged])

        train_summary_dir = os.path.join(self.output_dir, "summaries_" +  self.params.game)
        self.train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)

"""
This module contains class definitions for memories used during training of the
Q-network.
"""

import os
import numpy as np
import h5py
import shutil


class ScreenHistory(object):
    """A simple class to store the last N screen images in the batch. When we actually do
    train, we use the replay memory, this class is used to interact with the
    environment and obtain new rewards based on actions etc.

    Based on: https://github.com/devsisters/DQN-tensorflow/blob/master/dqn/history.py

    Parameters
    ----------
    params:
        arguments

    Attributes
    ----------
    screens : :obj: 'list' of :obj: 'ndarray' of :obj: 'float'
        The screen output (grayscale) collected in a small history with the
        newest in the highest index.

    """


    def __init__(self, params):

        self.history_length = params.history_length
        self.nchannels = params.nchannels

        if self.nchannels == 1:
            self.screens = np.zeros(
                [params.history_length, params.input_height, params.input_width], 
                dtype=np.float32
            )
        else:
            self.screens = np.zeros(
                [params.history_length, params.input_height, params.input_width, self.nchannels], 
                dtype=np.float32
            )



    def add(self, screen):
        """Adds a new screen to the screen buffer by first deleting the oldest
        screen in the buffer and then inserting.

        Returns
        -------
        None

        """

        self.screens = np.delete(self.screens, 0, axis=0)
        self.screens = np.insert(self.screens, self.history_length-1, screen, axis=0)


    def reset(self):
        """Reset the screen buffer.

        Returns
        -------
        None

        """
        self.screens *= 0.0


    def get(self):
        """Returns the screen buffer.

        If we have inputted arrays of size (2, 2), we have that self.screens
        is of (4, 2, 2). We then convert to (1, 2, 2, 4).
        So we have 4 channels with each corresponding to a time-step, and the
        other two dimensions just correspond to specific positions in the
        general image.

        Example
        -------
        >>> test = array([[[ 0.,  0.],
                [ 0.,  0.]],

               [[ 1.,  2.],
                [ 3.,  4.]],

               [[ 1.,  2.],
                [ 3.,  4.]],

               [[ 2.,  3.],
                [ 4.,  5.]]])

        >>> np.transpose(test, [1, 2, 0])
        array([[[ 0.,  1.,  1.,  2.],   --> (time-series for (0,0))
                [ 0.,  2.,  2.,  3.]],  --> (time-series for (0,1))

               [[ 0.,  3.,  3.,  4.],   --> (time-series for (1,0))
                [ 0.,  4.,  4.,  5.]]]) --> (time-sereis for (1,1))

        So in each row we will have a time-sequence at a position in the array
        and the highest dimension just corresponds to all the time-series for
        the row in the original.

        Returns
        -------
        :obj: 'list' of :obj: 'ndarray' of :obj: 'float'

        """

        # For mine-sweeper and others with no history
        if self.history_length == 4 or self.nchannels == 1: # This is for atari, need to clean it up
            screen = np.expand_dims(np.transpose(self.screens, [1, 2, 0]), axis=0)
        elif self.nchannels == 2:
            return self.screens

        return screen


class ReplayMemory(object):
    """A class for replay memories used during training of deep Q-networks.

    Based on https://github.com/devsisters/DQN-tensorflow/blob/master/dqn/replay_memory.py

    Parameters
    ----------
    replay_capacity : int
        The maximum number of samples in replay memory
    batch_size : Bool
        The number of frames to return.

    Attributes:
    replay_capacity : int
        The maximum number of samples in replay memory
    counter : int
        The next place in the replay buffer where we can place an element.
    current : int
        current position in the replay buffer, we wrap around when we reach
        the end.
    screens : :obj: 'ndarray' of :obj: 'float'
        The screen output (grayscale)
    rewards : :obj: 'list' of :obj: 'float'
        The screen output (grayscale)
    dones : :obj: 'list' of :obj: 'bool'
        The screen output (grayscale)
    batch_size : int
        The size of the batch
    batch_screens_pre : :obj: 'ndarray' of :obj: 'float'
        The set of screens prior to a corresponding set of actions.
    batch_screens_post : :obj: 'ndarray' of :obj: 'float'
        The set of screens after a corresponding set of actions.

    """


    def __init__(
        self, replay_capacity=int(1e6), history_length=4, nchannels=1, batch_size=32, 
        screen_height=84, screen_width=84, file_name = None,
        memory_checkpoint = int(1e6), restore_memory=False, output_dir = None
    ):

        self.replay_capacity = replay_capacity
        self.counter = 0
        self.current = 0

        self.history_length = history_length
        self.nchannels = nchannels

        self.output_dir = output_dir

        self.restore_memory = restore_memory

        self.memory_checkpoint = memory_checkpoint

        self.screen_height  = screen_height
        self.screen_width   = screen_width

        self.file_name = file_name

        # Pre-initialization for batches
        self.batch_size = batch_size


        # We do np.zeros since it seems faster than np.emtpy when filling, see
        # https://stackoverflow.com/questions/43145332/numpy-array-of-zeros-or-empty
        # https://stackoverflow.com/questions/26213199/np-fullsize-0-vs-np-zerossize-vs-np-empty

        if self.nchannels == 1:
            self.screens    = np.zeros(
                                [self.replay_capacity, screen_height, screen_width], 
                                dtype=np.float16
                            )

            # Change 4 to self.history_length
            self.batch_screens_pre  = np.zeros(
                                        [self.batch_size, self.history_length, screen_height, screen_width], 
                                        dtype=np.float16)

            self.batch_screens_post = np.zeros(
                                        [self.batch_size, self.history_length, screen_height, screen_width], 
                                        dtype=np.float16)

        else:
            # We have removed self.history_length for these, maybe add nchannels here and then do
            # the transpotition in the end

            self.screens    = np.zeros(
                                [self.replay_capacity, screen_height, screen_width, nchannels], 
                                dtype=np.float16
                            )

            # Change 4 to self.history_length
            self.batch_screens_pre  = np.zeros(
                                        [self.batch_size, screen_height, screen_width, nchannels], 
                                        dtype=np.float16)

            self.batch_screens_post = np.zeros(
                                        [self.batch_size, screen_height, screen_width, nchannels], 
                                        dtype=np.float16)


        self.actions    = np.zeros(self.replay_capacity, dtype=np.uint8)
        self.rewards    = np.zeros(self.replay_capacity, dtype=np.float16)
        self.dones      = np.zeros(self.replay_capacity, dtype=np.bool)

        if self.file_name:
            self.checkpoint_dir = os.path.abspath(os.path.join(self.output_dir, "replay_" + self.file_name))
            if not os.path.exists(self.checkpoint_dir):
                os.makedirs(self.checkpoint_dir)

        # Check if there is a replay buffer stored
        if os.path.exists(self.checkpoint_dir) and self.restore_memory:
            self.load_memory()



    def add(self, action, reward, post_screen, is_done):
        """Add a sample to the replay memory. The previous screen is already in
        the memory so only the screen after performing the action is required 
        (this is post_screen)

        Parameters
        ----------
        action: int
            The action performed.
        reward: float
            The reward gained form performing an action (clipped)
        post_screen: :obj: 'array' of :obj: 'float'
            The screen after performing an action.
        is_done: bool
            True for game is over, False otherwise.

        Returns
        -------
        None

        """
        self.rewards[self.current]      = reward
        self.actions[self.current]      = action

        # Same as [1,:,:] but faster
        self.screens[self.current, ...] = post_screen
        self.dones[self.current]        = is_done

        self.counter = max(self.counter, self.current+1)
        self.current = (self.current + 1) % self.replay_capacity

        #Check if we should save now
        # We use current since we also want to save newer episodes.
        if not (self.current + 1) % self.memory_checkpoint:
            self.save_memory()


    # Rename this to getState()
    def state(self, index):
        """Returns the current state of the game, that is the screens at this 
        index. This method is based on the following implementation: 
        https://goo.gl/1U2eFn

        Parameters
        ----------
        index: int
            index of the state in replay memory

        Returns
        -------
        None

        """

        assert self.counter > 0, "replay memory is empy, use at least --random_steps 1"

        # if is not in the beginning of matrix
        index = index % self.counter
        if index >= (self.history_length-1): # history length - 1
            # use faster slicing
            screens = self.screens[(index-(self.history_length-1)):(index + 1),...]
        else:
            # otherwise normalize indexes and use slower list based access
            indexes = [(index - i) % self.counter for i in reversed(range(self.history_length))]
            screens = self.screens[indexes,...]

        return screens

    def sample_batch(self):
        """Randomly samples a batch from the replay memory based on the
        given batch_size.
        This method also checks if the sample is already in the batch when
        doing random sampling.

        Returns
        -------
        :obj: 'list' of 
            :obj: 'list' of :obj: 'ndarray' of :obj: 'float'
                Sets of screens before an action.
            :obj: 'list' of :obj: 'int'
                Sets of actions.
            :obj: 'list' of :obj: 'float'
                Sets of rewards for these actions.
            :obj: 'list' of :obj: 'ndarray' of :obj: 'float'
                Sets of screens after an action.
            :obj: 'list' of :obj: 'bool'
                Sets containing if game finished.

        """

        assert self.counter > self.history_length
        assert self.counter > self.batch_size # We need to have more

        # Indices of samples in batch.
        samples_in_batch = []
        while len(samples_in_batch) < self.batch_size:

            # find random index
            while True:
                # sample one index (ignore states wraping over 
                index = np.random.randint(self.history_length, self.counter-1)
                if index >= self.current > (index-self.history_length):
                    # Sample not within range, continue
                    continue

                # if wraps over current pointer, then get new one
                if index >= self.current and index - self.history_length < self.current:
                    continue
                # Sample is already in the batch, get a new one
                if index in samples_in_batch:
                    continue

                # Check there is a done state within the index range
                # The last state can't be a done state
                if self.dones[(index-self.history_length):index].any():
                    # Found done state
                    continue

                # Found a valid sample
                break

            # Add the sample to the batch
            self.batch_screens_pre[len(samples_in_batch), ...]  = \
                 self.state(index-1)

            # ... is the same as :,:,:
            self.batch_screens_post[len(samples_in_batch), ...] = \
                 self.state(index)

            samples_in_batch.append(index)


        # Note that we permute axis of the tensors from [32,4,84,84] to 
        # [32,84,84,4]
        # In order to have it as channels for atari
        # This always has to be 3 for atari!
        # This is the same thing we do in screen history
        if self.nchannels == 1:
            return_screens_pre = np.transpose(self.batch_screens_pre, (0, 2, 3, 1))
            #print(return_screens_pre.shape)
            return_screens_post = np.transpose(self.batch_screens_post, (0, 2, 3, 1))

        else:
            return_screens_pre = self.batch_screens_pre
            return_screens_post = self.batch_screens_post

        actions = self.actions[samples_in_batch]
        rewards = self.rewards[samples_in_batch].astype(np.float16)
        dones = self.dones[samples_in_batch].astype(np.float16)

        return return_screens_pre, actions, rewards, return_screens_post, dones

    def num_examples(self):
        """int: Returns the number of examples."""
        return min(self.counter, self.replay_capacity)


    def action_counts(self):
        """ndarray of ints: Returns the number of bins in actions."""
        return np.bincount(self.actions)


    def total_reward(self):
        """float: Returns the total reward."""
        return np.sum(self.rewards)

    def save_memory(self):   
        """Saves a replay buffer to h5.

        Parameters
        ----------
        file_name: str
            The name of the game

        """
        print()
        print("Saving replay buffer to memory.")
        print()

        # Remove if it exists
        if os.path.exists(self.checkpoint_dir):
            shutil.rmtree(self.checkpoint_dir)
            os.makedirs(self.checkpoint_dir)
        else:
            os.makedirs(self.checkpoint_dir)

        with open(self.checkpoint_dir + os.sep + self.file_name + '.txt', 'w') as f:
            f.write('%d\n' % self.counter)
            f.write('%d\n' % self.current)

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_screens.h5', 'w') as hf:
            hf.create_dataset(self.file_name,  data=self.screens[0:(self.counter-1), ...], chunks=True, compression="gzip", compression_opts=4)

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_actions.h5', 'w') as hf:
            hf.create_dataset(self.file_name,  data=self.actions[0:(self.counter-1), ...], chunks=True, compression="gzip", compression_opts=4)

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_rewards.h5', 'w') as hf:
            hf.create_dataset(self.file_name,  data=self.rewards[0:(self.counter-1), ...], chunks=True, compression="gzip", compression_opts=4)

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_dones.h5', 'w') as hf:
            hf.create_dataset(self.file_name,  data=self.dones[0:(self.counter-1), ...], chunks=True, compression="gzip", compression_opts=4)

    def load_memory(self):
        """Loads a replay buffer from h5.

        Parameters
        ----------
        self.file_name: str
            The name of the game

        """

        print()
        print("Loading replay buffer from memory")
        print()

        with open(self.checkpoint_dir + os.sep + self.file_name + '.txt', 'r') as f:
            self.counter = int(f.readline())
            self.current = int(f.readline())

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_screens.h5', 'r') as hf:
            self.screens[0:(self.counter-1), ...] = hf[self.file_name][0:(self.counter-1)]

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_actions.h5', 'r') as hf:
            self.actions[0:(self.counter-1), ...] = hf[self.file_name][0:(self.counter-1)]

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_rewards.h5', 'r') as hf:
            self.rewards[0:(self.counter-1), ...] = hf[self.file_name][0:(self.counter-1)]

        with h5py.File(self.checkpoint_dir + os.sep + self.file_name + '_dones.h5', 'r') as hf:
            self.dones[0:(self.counter-1), ...] = hf[self.file_name][0:(self.counter-1)]



  
"""
This module contains class definitions for open ai gym environments.
"""

import os
import collections
import argparse
import random
from datetime import datetime
import time
from functools import reduce
import time

import numpy as np
import tensorflow as tf

# from environment import MinesweeperEnvironment


# from qnetwork import DeepQNetwork, update_target_network
# from replay_memory import ReplayMemory, ScreenHistory


class QAgent:
    """An environment class for open ai gym atari games using the screen.
    Attributes:
        _display : bool
            Display the game visually
        _screen (:obj: 'array', :obj: 'float') : The screen output (rgb)
        _reward (float) : amount of reward achieved by the previous action. 
                          The scale varies between environments, 
                          but the goal is always to increase your total reward.
        _done (bool) : Whether it's time to reset the environment again. 
                       Most (but not all) tasks are divided up into well-defined
                       episodes, and done being True indicates the episode has 
                       terminated.
        _random_start (int) : How long we let the agent take random actions in a
                              new game.
        screen_width (int) : The width of the screen after resizing.
        screen_height (int) : The height of the screen after resizing.
        _action_repeat (int) : The number of time-steps an action is repeated.
        env (:obj:) : The open ai gym environment object
    """

    def __init__(self, params):

        self.params = params # These are the parameters collected for the agent.

        # Load environmnet

        if self.params.env == 'atari':

            self.game = AtariGymEnvironment(
                            self.params.random_start_wait, 
                            self.params.show_game, 
                            self.params.input_height,
                            self.params.input_width,
                            self.params.game,
                            self.params.reward_recent_update)

        elif self.params.env == 'minesweeper':
            self.game = MinesweeperEnvironment(
                            self.params.input_height,
                            self.params.input_width,
                            self.params.mines_min,
                            self.params.mines_max,
                            self.params.show_game,
                            self.params.reward_recent_update)

        # Initialize two Q-Value Networks 
        # Q-network for training.

        self.dqn_train = DeepQNetwork(
                            params=self.params,
                            num_actions=self.game.num_actions,
                            network_name="qnetwork-train",
                            trainable=True)

        if self.params.is_train:

            # Q-Network for predicting target Q-values
            self.dqn_target = DeepQNetwork(
                                params=self.params,
                                num_actions=self.game.num_actions,
                                network_name="qnetwork-target",
                                trainable=False)
            
            # Initialize replay memory for storing experience to sample batches from
            self.replay_mem = ReplayMemory(
                                self.params.replay_capacity, 
                                self.params.history_length,
                                self.params.nchannels,
                                self.params.batch_size,
                                self.params.input_height,
                                self.params.input_width,
                                self.params.game,
                                self.params.memory_checkpoint,
                                self.params.restore_memory,
                                self.params.output_dir)

        # Small structure for storing the last four screens
        self.history = ScreenHistory(self.params)

        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it
        self.checkpoint_dir    = os.path.abspath(os.path.join(self.params.output_dir, "checkpoints_" + self.params.game))
        self.checkpoint_prefix = os.path.join(self.checkpoint_dir, "model")
        if not os.path.exists(self.checkpoint_dir):
            os.makedirs(self.checkpoint_dir)

        self.train_iteration    = 0
        self.count_actions      = np.zeros(self.game.num_actions)   # Count per action (only greedy)
        self.count_act_random   = 0  # Count of random actions
        self.count_act_greedy   = 0  # Count of greedy actions
        self.win_rate = 0.0 # For atari

        # Histories of qvalues and loss for running average
        self.qvalues_hist = collections.deque([0]*self.params.interval_summary,  maxlen=self.params.interval_summary)
        self.loss_hist    = collections.deque([10]*self.params.interval_summary, maxlen=self.params.interval_summary)

        self.epsilon = 0

    def fit(self):

        # Initialize a new game and store the screens in the self.history
        #screen, reward, is_done = self.game.new_random_game()
        if self.params.env == 'atari':
            screen, reward, is_done = self.game.new_random_game()
        else:
            screen, reward, is_done = self.game.new_game()
        for _ in range(self.params.history_length):
            self.history.add(screen)

        # Initialize the TensorFlow session
        gpu_options = tf.GPUOptions(
           per_process_gpu_memory_fraction=self.params.gpu_memory
        )

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:

            # Initialize the TensorFlow session
            init = tf.global_variables_initializer()
            sess.run(init)

            # Only save trainable variables and the global iteration to disk
            tf_vars_to_save = tf.trainable_variables() + [self.dqn_train.global_iteration]
            saver = tf.train.Saver(tf_vars_to_save, max_to_keep=200)

            if self.params.model_file is not None:
                # Load pre-trained model from disk
                model_path = os.path.join(self.checkpoint_dir, self.params.model_file)
                saver.restore(sess, model_path)
                self.train_iteration, learning_rate = sess.run([self.dqn_train.global_iteration, self.dqn_train.learning_rate])
                print("Restarted training from model file. iteration = %06i, Learning Rate = %.5f" % (self.train_iteration, learning_rate))

            # Initialize summary writer
            self.dqn_train.build_summary_writer(sess)

            # Initialize the target Q-Network fixed with the same weights
            update_target_network(sess, "qnetwork-train", "qnetwork-target")

            for iteration in range(self.params.num_iterations):
                self._sel_move(sess, iteration)
                self._train(sess, iteration, saver)

            print("Finished training Q-network.")

    def _sel_move(self, sess, iteration):

        if self.params.is_train:
            replay_mem_size = self.replay_mem.num_examples()
            if replay_mem_size < self.params.train_start and iteration % 1000 == 0:
                print("Initializing replay memory %i/%i" % (iteration, self.params.train_start))

        # self.epsilon Greedy Exploration: with the probability of self.epsilon
        # choose a random action, otherwise go greedy with the action
        # having the maximal Q-value. Note the minimum episolon of 0.1
        if self.params.is_train:
            self.epsilon = max(self.params.min_epsilon, 1.0-float(self.train_iteration*self.params.train_freq) / float(self.params.epsilon_step))
        else:
            self.epsilon = self.params.eval_epsilon

        ################################################################
        ####################### SELECT A MOVE ##########################
        ################################################################

        # Either choose a random action or predict the action using the Q-network
        do_random_action = (random.random() < self.epsilon)
        if do_random_action or (self.params.is_train and replay_mem_size < self.params.train_start):
            action_id = random.randrange(self.game.num_actions)
            self.count_act_random += 1
        else:

            # Get the last screens from the self.history and perform
            # feed-forward through the network to compute Q-values
            if self.params.network_type == 'conv':

                feed_dict  = { self.dqn_train.pl_screens: self.history.get() }

            elif self.params.network_type == 'fc':

                screen_reshape = np.reshape(self.history.get(), [1, self.params.input_width*self.params.input_height*self.params.nchannels])
                feed_dict  = { self.dqn_train.pl_flat: screen_reshape }

            qvalues    = sess.run(self.dqn_train.qvalues, feed_dict=feed_dict)

            # Choose the best action based on the approximated Q-values
            qvalue_max = np.max(qvalues[0])
            action_id  = np.argmax(qvalues[0])

            self.count_act_greedy += 1
            self.count_actions[action_id] += 1
            self.qvalues_hist.append(qvalue_max)

        self._move(action_id)


    def _move(self, action_id):

        ################################################################
        ####################### PLAY THE MOVE ##########################
        ################################################################

        # Play the selected action (either random or predicted) on the self.game game
        # Note that the action is performed for k = 4 frames (frame skipping)
        screen, cumulative_reward, is_done = self.game.act(action_id)

        # Perform reward clipping and add the example to the replay memory
        # This is done with Huber loss now
        #cumulative_reward = min(+1.0, max(-1.0, cumulative_reward))

        # Add the screen to short term self.history and replay memory
        self.history.add(screen)

        # Add experience to replay memory
        if self.params.is_train:
            self.replay_mem.add(action_id, cumulative_reward, screen, is_done)

        # Check if we are game over, and if yes, initialize a new game
        if is_done:
            if self.params.env == 'atari':
                screen, reward, is_done = self.game.new_random_game()
            else:
                screen, reward, is_done = self.game.new_game()

            if self.params.is_train:
                self.replay_mem.add(0, reward, screen, is_done)
                self.history.add(screen)

    def _train(self, sess, iteration, saver):

        ################################################################
        ###################### TRAINING MODEL ##########################
        ################################################################


        if self.params.is_train and iteration > self.params.train_start and iteration % self.params.train_freq == 0:

            screens, actions, rewards, screens_1, dones = self.replay_mem.sample_batch()

            # Below, we perform the Double-DQN update.

            if self.params.network_type == 'fc':
                screens = np.reshape(screens, [self.params.batch_size, self.params.input_width*self.params.input_height*self.params.nchannels])
                screens_1 = np.reshape(screens, [self.params.batch_size, self.params.input_width*self.params.input_height*self.params.nchannels])

                # The q-value from the target network is used 
                # we use the next screen
                qvalues_train = sess.run(
                    self.dqn_train.qvalues,
                    feed_dict={ self.dqn_train.pl_flat: screens_1 }
                )

                actions_target = tf.argmax(qvalues_train, 1) # Find the best actions for each using the train network
                # We use this to evalute the q-value for some state

                # Now,we get the q-values for all actions given the states
                # We then later sort out the q-values from the target network
                # using the best actions from the train network

                qvalues_target = sess.run(
                    self.dqn_target.qvalues,
                    feed_dict={ self.dqn_target.pl_flat: screens_1 }
                )

                # Inputs for trainable Q-network
                feed_dict = {
                    self.dqn_train.pl_flat   : screens,
                    self.dqn_train.pl_actions   : actions,
                    self.dqn_train.pl_rewards   : rewards,
                    self.dqn_train.pl_dones : dones,
                    #self.dqn_train.pl_qtargets  : np.max(qvalues_target, axis=1),
                    self.dqn_train.pl_qtargets  : qvalues_target,
                    self.dqn_train.pl_actions_target : actions_target,

                }

            elif self.params.network_type == 'conv':

                # First, we need to determine the best actions
                # in the train network
                qvalues_train = sess.run(
                    self.dqn_train.qvalues,
                    feed_dict={ self.dqn_train.pl_screens: screens_1 }
                )

                 # Find the best actions for each using the train network
                 # which will be used with the q-values form the target network
                actions_target = np.argmax(qvalues_train, 1)

                # We use this to evalute the q-value for some state
                # Now,we get the q-values for all actions given the states
                # We then later sort out the q-values from the target network
                # using the best actions from the train network

                qvalues_target = sess.run(
                    self.dqn_target.qvalues,
                    feed_dict={ self.dqn_target.pl_screens: screens_1 }
                )


                # Inputs for trainable Q-network
                feed_dict = {
                    self.dqn_train.pl_screens   : screens,
                    self.dqn_train.pl_actions   : actions,
                    self.dqn_train.pl_rewards   : rewards,
                    self.dqn_train.pl_dones : dones,
                    #self.dqn_train.pl_qtargets  : np.max(qvalues_target, axis=1),
                    self.dqn_train.pl_qtargets  : qvalues_target,
                    self.dqn_train.pl_actions_target : actions_target,
                }

            # Actual training operation
            _, loss, self.train_iteration = sess.run([self.dqn_train.train_op,
                                            self.dqn_train.loss,
                                            self.dqn_train.global_iteration],
                                            feed_dict=feed_dict)

            # Running average of the loss
            self.loss_hist.append(loss)

             # Check if the returned loss is not NaN
            if np.isnan(loss):
                print("[%s] Training failed with loss = NaN." %
                      datetime.now().strftime("%Y-%m-%d %H:%M"))

            # Once every n = 10000 frames update the Q-network for predicting targets
            if self.train_iteration % self.params.network_update_rate == 0:
                print("[%s] Updating target network." % datetime.now().strftime("%Y-%m-%d %H:%M"))
                update_target_network(sess, "qnetwork-train", "qnetwork-target")

            self._evaluate(sess, feed_dict)
            self._print_save(sess, feed_dict, saver)


    def _evaluate(self, sess, feed_dict):

        ################################################################
        ####################### MODEL EVALUATION #######################
        ################################################################

        if self.params.is_train and self.train_iteration % self.params.eval_frequency == 0 or self.train_iteration == 0:

            eval_total_reward = 0
            eval_num_episodes = 0
            eval_num_wins = 0
            eval_num_rewards = 0
            eval_episode_max_reward = 0
            eval_episode_reward = 0
            eval_actions = np.zeros(self.game.num_actions)


            # We store all of these parameters temporarily so this evaluation does not
            # affect model evaluation

            tmp_episode_step          = self.game._episode_step
            tmp_episode_number        = self.game._episode_number
            tmp_episode_reward        = self.game._episode_reward
            tmp_max_reward_episode    = self.game._max_reward_episode
            tmp_global_step           = self.game._global_step
            tmp_global_reward         = self.game._global_reward
            tmp_recent_reward         = self.game._recent_reward
            tmp_recent_episode_number = self.game._recent_episode_number
            tmp_recent_games_won      = self.game._recent_games_won
            tmp_games_won             = self.game._games_won
            tmp_reward_recent_update  = self.game.reward_recent_update

            prev_action_id = -1
            prev_episode_num = -1 # Just has to be different intially than prev
            action_id = -1 
            eval_num_episodes = 0

            # Initialize new game without random start moves
            screen, reward, done = self.game.new_game()

            for _ in range(self.params.history_length):
                self.history.add(screen)

            #for eval_iterations in range(self.params.eval_iterations):
            while eval_num_episodes < self.params.eval_iterations: # Play eval_iterations games
                prev_action_id = action_id

                # if random.random() < self.params.eval_epsilon:
                #     # Random action
                #     action_id = random.randrange(self.game.num_actions)
                #else:
                # Greedy action
                # Get the last screens from the self.history and perform
                # feed-forward through the network to compute Q-values
                feed_dict_eval  = { self.dqn_train.pl_screens: self.history.get() }
                qvalues = sess.run(self.dqn_train.qvalues, feed_dict=feed_dict_eval)

                # Choose the best action based on the approximated Q-values
                qvalue_max = np.max(qvalues[0])
                action_id  = np.argmax(qvalues[0])

                # Skip this action if we are in the same game
                if prev_action_id == action_id and prev_episode_num == eval_num_episodes:
                    action_id = random.randrange(self.game.num_actions)

                prev_episode_num = eval_num_episodes

                # Keep track of how many of each action is performed
                eval_actions[action_id] += 1

                # Perform the action
                screen, reward, done = self.game.act(action_id)
                self.history.add(screen)

                eval_episode_reward += reward
                if reward > 0:
                    eval_num_rewards += 1

                if reward == self.game.env.rewards["win"]:
                    eval_num_wins += 1

                if done:
                    # Note max reward is from playin gthe games
                    eval_total_reward += eval_episode_reward
                    eval_episode_max_reward = max(eval_episode_reward, eval_episode_max_reward)
                    eval_episode_reward = 0
                    eval_num_episodes += 1

                    screen, reward, done = self.game.new_game()
                    for _ in range(self.params.history_length):
                        self.history.add(screen)

            # Send statistics about the environment to TensorBoard
            eval_update_ops = [
                self.dqn_train.eval_rewards.assign(eval_total_reward),
                self.dqn_train.eval_win_rate.assign((eval_num_wins / eval_num_episodes)*100),
                self.dqn_train.eval_num_rewards.assign(eval_num_rewards),
                self.dqn_train.eval_max_reward.assign(eval_episode_max_reward),
                self.dqn_train.eval_num_episodes.assign(eval_num_episodes),
                self.dqn_train.eval_actions.assign(eval_actions / np.sum(eval_actions))

            ]
            sess.run(eval_update_ops)
            summaries = sess.run(self.dqn_train.eval_summary_op, feed_dict=feed_dict)
            self.dqn_train.train_summary_writer.add_summary(summaries, self.train_iteration)

            print("[%s] Evaluation Summary" % datetime.now().strftime("%Y-%m-%d %H:%M"))
            print("  Total Reward: %i" % eval_total_reward)
            print("  Max Reward per Episode: %i" % eval_episode_max_reward)
            print("  Num Episodes: %i" % eval_num_episodes)
            print("  Num Rewards: %i" % eval_num_rewards)
            print("  Win Rate: %.1f" % ((eval_num_wins / eval_num_episodes)*100))

            self.win_rate = (eval_num_wins / eval_num_episodes)*100


            self.game._episode_step          = tmp_episode_step
            self.game._episode_number        = tmp_episode_number
            self.game._episode_reward        = tmp_episode_reward
            self.game._max_reward_episode    = tmp_max_reward_episode
            self.game._global_step           = tmp_global_step
            self.game._global_reward         = tmp_global_reward
            self.game._recent_reward         = tmp_recent_reward
            self.game._recent_episode_number = tmp_recent_episode_number
            self.game._recent_games_won      = tmp_recent_games_won
            self.game._games_won             = tmp_games_won
            self.game.reward_recent_update   = tmp_reward_recent_update



    def _print_save(self, sess, feed_dict, saver):

        ################################################################
        ###################### PRINTING / SAVING #######################
        ################################################################

        # Write a training summary to disk
        # This is what controls how often we write to disk
        if self.params.is_train and self.train_iteration % self.params.interval_summary == 0:

            # Send statistics about the environment to TensorBoard
            update_game_stats_ops = [
                self.dqn_train.avg_reward_per_game.assign(self.game.avg_reward_per_episode()),
                self.dqn_train.max_reward_per_game.assign(self.game.max_reward_per_episode),
                self.dqn_train.avg_moves_per_game.assign(self.game.avg_steps_per_episode()),
                self.dqn_train.total_reward_replay.assign(self.replay_mem.total_reward()),
                self.dqn_train.num_games_played.assign(self.game.episode_number),
                self.dqn_train.moves.assign(self.game.global_step),
                self.dqn_train.actions_random.assign(self.count_act_random),
                self.dqn_train.actions_greedy.assign(self.count_act_greedy),
            ]
            sess.run(update_game_stats_ops)

            # Build and save summaries
            summaries = sess.run(self.dqn_train.train_summary_op, feed_dict=feed_dict)


            # Here we set train_iteration on x-axis
            self.dqn_train.train_summary_writer.add_summary(summaries, self.train_iteration)

            # Here we set number of moves on x-axis
            #self.dqn_train.train_summary_writer.add_summary(summaries, self.game.global_step)

            avg_qvalue = avg_loss = 0
            for i in range(len(self.qvalues_hist)):
                avg_qvalue += self.qvalues_hist[i]
                avg_loss   += self.loss_hist[i]

            avg_qvalue /= float(len(self.qvalues_hist))
            avg_loss   /= float(len(self.loss_hist))

            learning_rate = sess.run(self.dqn_train.learning_rate)

            format_str = "[%s] It. %06i, Replay = %i, epsilon = %.4f, "\
                         "Episodes = %i, Steps = %i, Avg.R = %.3f, "\
                         "Max.R = %.3f, Win = %.1f, Avg.Q = %.4f, Avg.Loss = %.6f, lr = %.6f"
            print(format_str % (datetime.now().strftime("%Y-%m-%d %H:%M"), self.train_iteration,
                                self.replay_mem.num_examples(), self.epsilon, self.game.episode_number, self.game.global_step,
                                self.game.avg_reward_per_episode(),
                                self.game.max_reward_per_episode, self.win_rate,
                                avg_qvalue, avg_loss, learning_rate))

        # Write model checkpoint to disk
        if self.params.is_train and self.train_iteration % self.params.interval_checkpoint == 0:
            path = saver.save(sess, self.checkpoint_prefix, global_step=self.train_iteration)
            print("[%s] Saving TensorFlow model checkpoint to disk." %
                  datetime.now().strftime("%Y-%m-%d %H:%M"))

            sum_actions = float(reduce(lambda x, y: x+y, self.count_actions))
            action_str = ""
            for action_id, action_count in enumerate(self.count_actions):
                action_perc = action_count/sum_actions if not sum_actions == 0 else 0
                action_str += "<%i, %s, %i, %.2f> " % \
                              (action_id, self.game.action_to_string(action_id),
                               action_count, action_perc)

            format_str = "[%s] Q-Network Actions Summary: NumRandom: %i, NumGreedy: %i, %s"
            print(format_str % (datetime.now().strftime("%Y-%m-%d %H:%M"),
                                self.count_act_random, self.count_act_greedy, action_str))


    def play(self):

        # Initialize a new game and store the screens in the self.history
        screen, reward, is_done = self.game.new_game()
        for _ in range(self.params.history_length):
            self.history.add(screen)

        # Initialize the TensorFlow session
        gpu_options = tf.GPUOptions(
           per_process_gpu_memory_fraction=self.params.gpu_memory
        )

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:

            # Initialize the TensorFlow session
            init = tf.global_variables_initializer()
            sess.run(init)

            # Only save trainable variables and the global step to disk
            tf_vars_to_save = tf.trainable_variables() + [self.dqn_train.global_iteration]
            saver = tf.train.Saver(tf_vars_to_save, max_to_keep=200)

            if self.params.model_file is not None:
                # Load pre-trained model from disk
                model_path = os.path.join(self.checkpoint_dir, self.params.model_file)
                saver.restore(sess, model_path)

            for step in range(self.params.num_steps):
                self._sel_move(sess, step)
                if self.params.show_game:
                    time.sleep(0.04)

    def play_mine(self):

        # Initialize a new game and store the screens in the self.history
        screen, reward, is_done = self.game.new_game()
        for _ in range(self.params.history_length):
            self.history.add(screen)

        # Initialize the TensorFlow session
        gpu_options = tf.GPUOptions(
           per_process_gpu_memory_fraction=self.params.gpu_memory
        )

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:

            # Initialize the TensorFlow session
            init = tf.global_variables_initializer()
            sess.run(init)

            # Only save trainable variables and the global iteration to disk
            tf_vars_to_save = tf.trainable_variables() + [self.dqn_train.global_iteration]
            saver = tf.train.Saver(tf_vars_to_save, max_to_keep=200)

            if self.params.model_file is not None:
                # Load pre-trained model from disk
                model_path = os.path.join(self.checkpoint_dir, self.params.model_file)
                saver.restore(sess, model_path)


            while self.game.episode_number < self.params.num_games:
                if self.params.show_game:
                    inp = input("Enter input (ROW,COL)")
                self._sel_move(sess, 0)

            print(self.game.episode_number)

            print(self.game.win_rate)



    def evaluate_ms(self):

        # Initialize a new game and store the screens in the self.history
        screen, reward, is_done = self.game.new_game()
        for _ in range(self.params.history_length):
            self.history.add(screen)

        # Initialize the TensorFlow session
        gpu_options = tf.GPUOptions(
           per_process_gpu_memory_fraction=self.params.gpu_memory
        )

        with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:
            max_name = 800000
            min_name = 680000
            current_name = min_name
            best_model = min_name
            best_win_rate = 0
            current_win_rate = 0

            # Initialize the TensorFlow session
            init = tf.global_variables_initializer()
            sess.run(init)
            # Only save trainable variables and the global iteration to disk
            tf_vars_to_save = tf.trainable_variables() + [self.dqn_train.global_iteration]
            saver = tf.train.Saver(tf_vars_to_save, max_to_keep=200)


            while current_name <= max_name:

                print("Restoring: ", current_name)


                # if self.params.model_file is not None:
                #     # Load pre-trained model from disk
                #     model_path = os.path.join(self.checkpoint_dir, self.params.model_file)
                #     saver.restore(sess, model_path)
                model_path = os.path.join(self.checkpoint_dir, 'model-' + str(current_name))
                saver.restore(sess, model_path)

                prev_action_id = -1
                prev_episode_num = -1 # Just has to be different intially than prev
                action_id = -1 
                eval_num_episodes = 0

                eval_total_reward = 0
                eval_num_episodes = 0
                eval_num_wins = 0
                eval_num_rewards = 0
                eval_episode_max_reward = 0
                eval_episode_reward = 0
                eval_actions = np.zeros(self.game.num_actions)

                # Initialize new game without random start moves
                screen, reward, done = self.game.new_game()

                for _ in range(self.params.history_length):
                    self.history.add(screen)

                #for eval_iterations in range(self.params.eval_iterations):
                while eval_num_episodes < self.params.eval_iterations: # Play eval_iterations games
                    prev_action_id = action_id

                    feed_dict_eval  = { self.dqn_train.pl_screens: self.history.get() }
                    qvalues = sess.run(self.dqn_train.qvalues, feed_dict=feed_dict_eval)

                    # Choose the best action based on the approximated Q-values
                    qvalue_max = np.max(qvalues[0])
                    action_id  = np.argmax(qvalues[0])

                    # Skip this action if we are in the same game
                    if prev_action_id == action_id and prev_episode_num == eval_num_episodes:
                        action_id = random.randrange(self.game.num_actions)

                    prev_episode_num = eval_num_episodes


                    # Perform the action
                    screen, reward, done = self.game.act(action_id)
                    self.history.add(screen)

                    eval_episode_reward += reward
                    if reward > 0:
                        eval_num_rewards += 1

                    if reward == self.game.env.rewards["win"]:
                        eval_num_wins += 1

                    if done:
                        # Note max reward is from playin gthe games
                        eval_total_reward += eval_episode_reward
                        eval_episode_max_reward = max(eval_episode_reward, eval_episode_max_reward)
                        eval_episode_reward = 0
                        eval_num_episodes += 1

                        screen, reward, done = self.game.new_game()
                        for _ in range(self.params.history_length):
                            self.history.add(screen)

                current_win_rate = (eval_num_wins / eval_num_episodes)*100

                print("  Win Rate: %.2f" % (current_win_rate))

                if current_win_rate > best_win_rate:
                    best_win_rate = current_win_rate
                    best_model = current_name

                current_name = current_name + 20000

            print("Best model is: ", best_model)



"""
This module contains class definitions for open ai gym environments.
"""

import os
import collections
import argparse
import random
from datetime import datetime
import time
from functools import reduce

import numpy as np
import tensorflow as tf

# from qnetwork import DeepQNetwork, update_target_network
# from replay_memory import ReplayMemory, ScreenHistory
# from agent import QAgent

import random

def train(params):

    # https://stackoverflow.com/questions/11526975/set-random-seed-programwide-in-python
    # https://stackoverflow.com/questions/30517513/global-seed-for-multiple-numpy-imports
    random.seed(params.seed)
    np.random.seed(params.seed)
    # Must be called before Session
    # https://stackoverflow.com/questions/38469632/tensorflow-non-repeatable-results/40247201#40247201
    tf.set_random_seed(params.seed)

    qagent = QAgent(params)
    if params.is_train:
        qagent.fit()
    elif params.env == 'atari':
        qagent.play()
    elif params.env == 'minesweeper':
        qagent.evaluate_ms()

# View tensorboard with 
# tensorboard --logdir output



class Params:
  def __init__(self):
    self.seed = 0
    self.env = "minesweeper"
    self.input_height = 6
    self.input_width = 6
    self.game="minesweeper"
    self.mines_min = 6
    self.mines_max = 6
    self.history_length = 1
    self.train_freq = 1
    self.nchannels = 2
    self.batch_size = 400
    self.memory_checkpoint = int(5e5)
    self.train_start = int(8e5)
    self.replay_capacity = int(1e6)
    self.interval_checkpoint = int(2e4)
    self.eval_frequency = 20000
    self.eval_iterations = 1000
    self.network_type='conv'
    self.min_epsilon = 0.1
    self.epsilon_step = 2.5e5
    self.discount = 0.0
    self.learning_rate = 0.00004
    self.learning_rate_step = 20000
    self.learning_rate_minimum = 0.00004
    self.learning_rate_decay = 0.90
    self.network_update_rate = int(1e5)
    self.eval_iterations = 1000
    self.eval_frequency = 20000
    self.reward_recent_update = int(1e5)
    self.num_steps = 500
    self.num_games = 10000
    self.eval_iterations = 10000
    self.is_train = True
    self.show_game = False
    self.model_file = None
    self.output_dir = "./drive/My Drive/Reinforcement/minesweeper_solver/q_learning/output/"
    self.filter_sizes = "8,4,3"
    self.filter_strides = "4,2,1"
    self.num_filters = "32,64,64"
    self.num_hidden          = 512      # 512
    self.num_actions         = 6*6            # depends on game
    self.dueling_type        = None
    self.clip_delta          = 1.0             # 1.0
    self.batch_accumulator   = "mean"     # mean or sum
    self.bias_init           = 0.01
    self.restore_memory = False
    self.interval_summary = 200
    self.gpu_memory = 0.5
    self.num_iterations = 50000000




if __name__ == "__main__":
    params = {
      'rom_file' : "./roms/Breakout.bin",
      'seed': 0,
    }
    #print(params.seed)
    parser = argparse.ArgumentParser(prog="train.py", description="Train Deep Q-Network for Atari games")
    #parser = argparse.ArgumentParser(description='Train Deep Q-Network for Atari games')
    # Atari ROM, TensorFlow model and output directory
    params['rom_file'] = "./roms/Breakout.bin"

    parser.add_argument('--rom', dest='rom_file', default="./roms/Breakout.bin", type=str, help="path to atari rom (.bin) file")
    parser.add_argument('--model', dest='model_file', type=str, required=False, help="path to TensorFlow model file")
    parser.add_argument('--out', dest='output_dir', type=str, default="./output/", help="output path models and screen captures")

    parser.add_argument('--train', dest="is_train", action="store_true", help="training or only playing")
    parser.add_argument('--randomstart', dest='random_start_wait', type=int, default=30, help="random number of frames to wait at start of episode")
    parser.add_argument('--game', dest='game', type=str, default="DemonAttack-v0", help="The game we play")
    parser.add_argument('--env', dest='env', type=str, default="atari", help="If we want to use atari or minesweeper")


    parser.add_argument('--gpumemory', dest="gpu_memory", type=float, default=0.5, help="The percentage of GPU memory allowed to be used by Tensorflow")

    # Parameters network input (screens)
    parser.add_argument('--inputheight', dest="input_height", type=int, default=84, help="screen input height")
    parser.add_argument('--inputwidth', dest="input_width", type=int, default=84, help="screen input width")    
    parser.add_argument('--historylength', dest="history_length", type=int, default=4, help="Numbe of moves which are repeated in atari")
    parser.add_argument('--mines-min', dest="mines_min", type=int, default=5, help="The number of mines")
    parser.add_argument('--mines-max', dest="mines_max", type=int, default=7, help="The number of mines")
    parser.add_argument('--nchannels', dest="nchannels", type=int, default=4, help="screen input depth")


    parser.add_argument('--network-type', dest='network_type', type=str, default="conv", help="conv|fc")


    # Parameters CNN architecture
    parser.add_argument('--filtersizes', dest="filter_sizes", type=str, default="8,4,3", help="CNN filter sizes")
    parser.add_argument('--filterstrides', dest="filter_strides", type=str, default="4,2,1", help="CNN filter strides")
    parser.add_argument('--numfilters', dest="num_filters", type=str, default="32,64,64", help="CNN number of filters per layer")
    parser.add_argument('--numhidden', dest="num_hidden", type=int, default=512, help="CNN number of neurons in FC layer")
    parser.add_argument('--duelingtype', dest="dueling_type", default=None, type=str, help="Type of dueling enabled")
    # See 
    # http://cs231n.github.io/neural-networks-2/
    parser.add_argument('--bias-init', dest="bias_init", type=float, default=0.01, help="The initial value of the biases")


    # Parameters for training the CNN
    parser.add_argument('--num-iterations', dest="num_iterations", type=int, default=50000000, help="Number of training iterations, i.e., number of passes, each pass using [batch size] number of examples")
    parser.add_argument('--batchsize', dest="batch_size", type=int, default=32, help="training batch size")
    parser.add_argument('--trainfreq', dest="train_freq", type=int, default=4, help="training frequency, default every frame")
    parser.add_argument('--epsilonstep', dest="epsilon_step", type=float, default=1e6, help="epsilon decrease step, linear annealing over iterations")
    parser.add_argument('--learnrate', dest="learning_rate", type=float, default=0.00025, help="optimization learning rate")
    parser.add_argument('--learnratedecay', dest="learning_rate_decay", type=float, default=0.98, help="learning rate decay")
    parser.add_argument('--learnratestep', dest="learning_rate_step", type=float, default=100000, help="learning rate decay step over iterations")
    parser.add_argument('--learnratemin', dest="learning_rate_minimum", type=float, default=0.0001, help="minimum learning rate")
    parser.add_argument('--discount', dest="discount", type=float, default=0.99, help="gamma for future discounted rewards")
    parser.add_argument('--clipdelta', dest="clip_delta", type=bool, default=True, help="clipping of error term in loss function")
    parser.add_argument('--networkupdate', dest="network_update_rate", type=float, default=10000, help="number of steps after which the Q-network is copied for predicting targets")
    parser.add_argument('--batchaccumulator', dest="batch_accumulator", type=str, default="mean", help="batch accumulator in loss function (mean or sum)")

    parser.add_argument('--replaycap', dest="replay_capacity", type=int, default=int(1e6), help="maximum number of samples in replay memory")
    parser.add_argument('--trainstart', dest="train_start", type=int, default=50000, help="start training when replay memory is of this size")

    # Parameters for evaluation of the model
    parser.add_argument('--evalfreq', dest="eval_frequency", type=int, default=250000, help="frequency of model evaluation")
    parser.add_argument('--evaliterations', dest="eval_iterations", type=int, default=125000, help="number of game iterations for each evaluation")
    parser.add_argument('--evalepsilon', dest="eval_epsilon", type=float, default=0.05, help="epsilon random move when evaluating")
    parser.add_argument('--minepsilon', dest="min_epsilon", type=float, default=0.1, help="Lowest epsilon when exploring")
    parser.add_argument('--num-steps', dest="num_steps", type=int, default=5000, help="Number of test steps when playing, each step is an action")
    parser.add_argument('--reward-recent', dest="reward_recent", type=int, default=1000, help="The number of episodes before resetting recent reward")
    parser.add_argument('--num-games', dest="num_games", type=int, default=5000, help="Number of test games to play minesweeper")


    # Parameters for outputting/debugging
    parser.add_argument('--intsummary', dest="interval_summary", type=int, default=200, help="frequency of adding training summaries, currently depending on train_iteration")
    parser.add_argument('--intcheckpoint', dest="interval_checkpoint", type=int, default=10000, help="frequency of saving model checkpoints")
    parser.add_argument('--memorycheckpoint', dest="memory_checkpoint", type=int, default=int(1e5), help="Frequency of saving memory based on addition counter.")
    parser.add_argument('--restore-memory', dest="restore_memory", type=bool, default=False, help="If True, restore replay memory.")
    parser.add_argument('--show', dest="show_game", action="store_true", help="show the Atari game output")
    parser.add_argument('--seed', dest="seed", type=int, default=0, help="The random seed value. Default at 0 means deterministic for all ops in Tensorflow 1.4")


    # Parse command line arguments and run the training process


    parser.set_defaults(game="minesweeper")

    parser.set_defaults(env='minesweeper')
    parser.set_defaults(mines_min=6)
    parser.set_defaults(mines_max=6)

    parser.set_defaults(input_width=6)
    parser.set_defaults(input_height=6)
    parser.set_defaults(history_length=1)
    parser.set_defaults(train_freq=1) # This should be the same as history length
    parser.set_defaults(nchannels=2)

    parser.set_defaults(batch_size=400)
    #parser.set_defaults(restore_memory=True)
    parser.set_defaults(memory_checkpoint=int(5e5))
    parser.set_defaults(train_start=int(8e5)) # Needs to be larger than batch-size, if reloading set to 0.
    #parser.set_defaults(train_start=int(5e4)) # Needs to be larger than batch-size, if reloading set to 0.
    #parser.set_defaults(train_start=int(100))
    parser.set_defaults(replay_capacity=int(1e6))
    parser.set_defaults(interval_checkpoint=int(2e4))

    parser.set_defaults(eval_frequency=20000)
    parser.set_defaults(eval_iterations=1000) # Changed to number of games player in minesweeper
    parser.set_defaults(reward_recent_update=int(1e5))

    parser.set_defaults(discount=0.0)
    #parser.set_defaults(learning_rate=0.00025/4)
    #parser.set_defaults(learning_rate=0.00025)
    parser.set_defaults(learning_rate=0.00004)
    #parser.set_defaults(learning_rate=0.001)
    #parser.set_defaults(learning_rate_step=50000)
    parser.set_defaults(learning_rate_step=20000)
    parser.set_defaults(learning_rate_decay=0.90)
    #parser.set_defaults(learning_rate_minimum=0.00025/4)
    parser.set_defaults(learning_rate_minimum=0.00004)
    parser.set_defaults(network_update_rate=int(1e5))
    parser.set_defaults(min_epsilon=0.1)
    parser.set_defaults(epsilon_step=2.5e5)
    #parser.set_defaults(eval_epsilon=0.001) # For exploration

    parser.set_defaults(network_type='conv')
    #parser.set_defaults(clip_delta=True) # This should be False for minesweeper, it seems
    #parser.set_defaults(dueling_type="mean") # Without this and with fc, the same network as Jacob


    # If we want to play
    parser.set_defaults(num_steps=500) # Number of steps to play atarai
    parser.set_defaults(num_games=10000) # Number of games to play in minesweeper

    #parser.set_defaults(model_file="model-1880000")
    #parser.set_defaults(model_file="model-1680000")
    #parser.set_defaults(model_file="model-1700000")
    #parser.set_defaults(model_file="model-1720000")
    parser.set_defaults(eval_iterations=10000) # For finally testing the best model
    parser.set_defaults(is_train=True) # Note, something is wrong with the play code!!!
    parser.set_defaults(show_game=False)



    params = Params()
    #params = parser.parse_args()
    #print(params.seed)


    train(params)

# review solution

#from google.colab import drive
#drive.mount('/content/drive')
#os.chdir('/content/drive/My Drive/Reinforcement')
! ls "/content/drive/My Drive/Reinforcement/minesweeper_solver/q_learning/output/checkpoints_minesweeper"
! pwd
#!git clone https://github.com/jakejhansen/minesweeper_solver.git
! ls

#sys.path.append('../')
#os.chdir('/content/drive/My Drive/Reinforcement/minesweeper_solver/q_learning/backup_output_net1_discount_0_batch_400_best')
!ls
#tf.enable_eager_execution()
#tf.executing_eagerly()
#tf.disable_v2_behavior()
#!python train.py



